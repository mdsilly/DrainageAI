{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DrainageAI Workflow\n",
    "\n",
    "Complete workflow for DrainageAI: setup, indices calculation, BYOL training, and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from google.colab import files\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rasterio geopandas scikit-image matplotlib tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/imagery\n",
    "!mkdir -p data/indices\n",
    "!mkdir -p results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Upload your multispectral imagery file (GeoTIFF format):\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Save uploaded files\n",
    "for filename in uploaded.keys():\n",
    "    with open(f\"data/imagery/{filename}\", 'wb') as f:\n",
    "        f.write(uploaded[filename])\n",
    "    print(f\"Saved {filename} to data/imagery/\")\n",
    "    \n",
    "# Get the first uploaded file\n",
    "imagery_filename = list(uploaded.keys())[0]\n",
    "imagery_path = f\"data/imagery/{imagery_filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check image properties\n",
    "with rasterio.open(imagery_path) as src:\n",
    "    num_bands = src.count\n",
    "    height = src.height\n",
    "    width = src.width\n",
    "    crs = src.crs\n",
    "    transform = src.transform\n",
    "    \n",
    "print(f\"Image has {num_bands} bands, dimensions: {width}x{height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Spectral Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_indices(imagery_path, output_path, indices=\"ndvi,msavi2\", \n",
    "                     red_band=3, nir_band=4, swir_band=5, green_band=2):\n",
    "    print(f\"Calculating spectral indices for {imagery_path}...\")\n",
    "    indices_to_calculate = indices.lower().split(\",\")\n",
    "    \n",
    "    with rasterio.open(imagery_path) as src:\n",
    "        num_bands = src.count\n",
    "        red_band = min(red_band, num_bands)\n",
    "        nir_band = min(nir_band, num_bands) if num_bands >= 4 else red_band\n",
    "        green_band = min(green_band, num_bands)\n",
    "        swir_band = min(swir_band, num_bands) if num_bands >= 5 else None\n",
    "        \n",
    "        red = src.read(red_band)\n",
    "        nir = src.read(nir_band)\n",
    "        green = src.read(green_band) if \"ndwi\" in indices_to_calculate else None\n",
    "        swir = src.read(swir_band) if \"ndmi\" in indices_to_calculate and swir_band is not None else None\n",
    "        meta = src.meta.copy()\n",
    "    \n",
    "    calculated_indices = []\n",
    "    band_names = []\n",
    "    \n",
    "    if \"ndvi\" in indices_to_calculate:\n",
    "        ndvi = (nir - red) / (nir + red + 1e-8)\n",
    "        calculated_indices.append(ndvi)\n",
    "        band_names.append(\"NDVI\")\n",
    "    \n",
    "    if \"ndmi\" in indices_to_calculate and swir is not None:\n",
    "        ndmi = (nir - swir) / (nir + swir + 1e-8)\n",
    "        calculated_indices.append(ndmi)\n",
    "        band_names.append(\"NDMI\")\n",
    "    \n",
    "    if \"msavi2\" in indices_to_calculate:\n",
    "        msavi2 = (2 * nir + 1 - np.sqrt((2 * nir + 1)**2 - 8 * (nir - red))) / 2\n",
    "        calculated_indices.append(msavi2)\n",
    "        band_names.append(\"MSAVI2\")\n",
    "    \n",
    "    if \"ndwi\" in indices_to_calculate and green is not None:\n",
    "        ndwi = (green - nir) / (green + nir + 1e-8)\n",
    "        calculated_indices.append(ndwi)\n",
    "        band_names.append(\"NDWI\")\n",
    "    \n",
    "    if not calculated_indices:\n",
    "        print(\"No indices were calculated.\")\n",
    "        return None\n",
    "    \n",
    "    indices_stack = np.stack(calculated_indices)\n",
    "    meta.update({\n",
    "        'count': len(calculated_indices),\n",
    "        'dtype': 'float32'\n",
    "    })\n",
    "    \n",
    "    with rasterio.open(output_path, 'w', **meta) as dst:\n",
    "        for i, (index, name) in enumerate(zip(calculated_indices, band_names), 1):\n",
    "            dst.write(index.astype(np.float32), i)\n",
    "            dst.set_band_description(i, name)\n",
    "    \n",
    "    print(f\"Indices saved to {output_path}\")\n",
    "    return indices_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate indices\n",
    "indices_path = \"data/indices/spectral_indices.tif\"\n",
    "indices = calculate_indices(\n",
    "    imagery_path=imagery_path,\n",
    "    output_path=indices_path,\n",
    "    indices=\"ndvi,msavi2\",\n",
    "    red_band=3,\n",
    "    nir_band=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BYOL Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BYOLProjector(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim=4096, out_dim=256):\n",
    "        super(BYOLProjector, self).__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.projection(x)\n",
    "\n",
    "class BYOLPredictor(nn.Module):\n",
    "    def __init__(self, in_dim=256, hidden_dim=4096, out_dim=256):\n",
    "        super(BYOLPredictor, self).__init__()\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.predictor(x)\n",
    "\n",
    "class BYOLModel(nn.Module):\n",
    "    def __init__(self, pretrained=True, with_indices=True, momentum=0.99):\n",
    "        super(BYOLModel, self).__init__()\n",
    "        \n",
    "        self.online_encoder = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        if with_indices:\n",
    "            first_conv = self.online_encoder.conv1\n",
    "            new_conv = nn.Conv2d(\n",
    "                in_channels=3 + 2,  # RGB + 2 indices\n",
    "                out_channels=64,\n",
    "                kernel_size=7,\n",
    "                stride=2,\n",
    "                padding=3,\n",
    "                bias=False\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                new_conv.weight[:, :3] = first_conv.weight\n",
    "                nn.init.kaiming_normal_(\n",
    "                    new_conv.weight[:, 3:],\n",
    "                    mode='fan_out',\n",
    "                    nonlinearity='relu'\n",
    "                )\n",
    "            \n",
    "            self.online_encoder.conv1 = new_conv\n",
    "        \n",
    "        self.online_encoder = nn.Sequential(*list(self.online_encoder.children())[:-1])\n",
    "        self.online_projector = BYOLProjector(2048)\n",
    "        self.predictor = BYOLPredictor()\n",
    "        self.target_encoder = copy.deepcopy(self.online_encoder)\n",
    "        self.target_projector = copy.deepcopy(self.online_projector)\n",
    "        \n",
    "        for param in self.target_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.target_projector.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.prediction_head = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.fine_tuned = False\n",
    "        self.with_indices = with_indices\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.fine_tuned:\n",
    "            features = self.online_encoder(x)\n",
    "            features = torch.flatten(features, 1)\n",
    "            return self.prediction_head(features)\n",
    "        else:\n",
    "            features = self.online_encoder(x)\n",
    "            features = torch.flatten(features, 1)\n",
    "            return features\n",
    "    \n",
    "    def byol_forward(self, x):\n",
    "        online_features = self.online_encoder(x)\n",
    "        online_features = torch.flatten(online_features, 1)\n",
    "        online_proj = self.online_projector(online_features)\n",
    "        online_pred = self.predictor(online_proj)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            target_features = self.target_encoder(x)\n",
    "            target_features = torch.flatten(target_features, 1)\n",
    "            target_proj = self.target_projector(target_features)\n",
    "        \n",
    "        return online_pred, target_proj, online_features\n",
    "    \n",
    "    def byol_loss(self, view1, view2):\n",
    "        online_pred1, target_proj2, online_feat1 = self.byol_forward(view1)\n",
    "        online_pred2, target_proj1, online_feat2 = self.byol_forward(view2)\n",
    "        \n",
    "        online_pred1 = F.normalize(online_pred1, dim=-1)\n",
    "        online_pred2 = F.normalize(online_pred2, dim=-1)\n",
    "        target_proj1 = F.normalize(target_proj1, dim=-1)\n",
    "        target_proj2 = F.normalize(target_proj2, dim=-1)\n",
    "        \n",
    "        loss1 = 2 - 2 * (online_pred1 * target_proj2).sum(dim=-1).mean()\n",
    "        loss2 = 2 - 2 * (online_pred2 * target_proj1).sum(dim=-1).mean()\n",
    "        \n",
    "        loss = (loss1 + loss2) / 2\n",
    "        return loss, (online_feat1 + online_feat2) / 2\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        for online_params, target_params in zip(\n",
    "            self.online_encoder.parameters(), self.target_encoder.parameters()\n",
    "        ):\n",
    "            target_params.data = self.momentum * target_params.data + \\\n",
    "                                (1 - self.momentum) * online_params.data\n",
    "        \n",
    "        for online_params, target_params in zip(\n",
    "            self.online_projector.parameters(), self.target_projector.parameters()\n",
    "        ):\n",
    "            target_params.data = self.momentum * target_params.data + \\\n",
    "                                (1 - self.momentum) * online_params.data\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save({\n",
    "            'online_encoder': self.online_encoder.state_dict(),\n",
    "            'online_projector': self.online_projector.state_dict(),\n",
    "            'predictor': self.predictor.state_dict(),\n",
    "            'target_encoder': self.target_encoder.state_dict(),\n",
    "            'target_projector': self.target_projector.state_dict(),\n",
    "            'prediction_head': self.prediction_head.state_dict(),\n",
    "            'fine_tuned': self.fine_tuned,\n",
    "            'with_indices': self.with_indices,\n",
    "            'momentum': self.momentum\n",
    "        }, path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.online_encoder.load_state_dict(checkpoint['online_encoder'])\n",
    "        self.online_projector.load_state_dict(checkpoint['online_projector'])\n",
    "        self.predictor.load_state_dict(checkpoint['predictor'])\n",
    "        self.target_encoder.load_state_dict(checkpoint['target_encoder'])\n",
    "        self.target_projector.load_state_dict(checkpoint['target_projector'])\n",
    "        self.prediction_head.load_state_dict(checkpoint['prediction_head'])\n",
    "        self.fine_tuned = checkpoint['fine_tuned']\n",
    "        self.with_indices = checkpoint.get('with_indices', True)\n",
    "        self.momentum = checkpoint.get('momentum', 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiViewDataset(Dataset):\n",
    "    def __init__(self, imagery_paths, indices_paths=None, transform=None):\n",
    "        self.imagery_paths = imagery_paths\n",
    "        self.indices_paths = indices_paths if indices_paths is not None else [None] * len(imagery_paths)\n",
    "        self.transform = transform\n",
    "        \n",
    "        assert len(self.imagery_paths) == len(self.indices_paths), \"Path lists must have the same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imagery_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with rasterio.open(self.imagery_paths[idx]) as src:\n",
    "            imagery = src.read()\n",
    "        \n",
    "        indices = None\n",
    "        if self.indices_paths[idx] is not None:\n",
    "            with rasterio.open(self.indices_paths[idx]) as src:\n",
    "                indices = src.read()\n",
    "        \n",
    "        imagery = torch.from_numpy(imagery).float()\n",
    "        if indices is not None:\n",
    "            indices = torch.from_numpy(indices).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            imagery_view1 = self.transform(imagery)\n",
    "            imagery_view2 = self.transform(imagery)\n",
    "            \n",
    "            if indices is not None:\n",
    "                indices_view1 = self.transform(indices)\n",
    "                indices_view2 = self.transform(indices)\n",
    "                \n",
    "                view1 = torch.cat([imagery_view1, indices_view1], dim=0)\n",
    "                view2 = torch.cat([imagery_view2, indices_view2], dim=0)\n",
    "            else:\n",
    "                view1 = imagery_view1\n",
    "                view2 = imagery_view2\n",
    "        else:\n",
    "            if indices is not None:\n",
    "                view1 = view2 = torch.cat([imagery, indices], dim=0)\n",
    "            else:\n",
    "                view1 = view2 = imagery\n",
    "        \n",
    "        return view1, view2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentations\n",
    "class RandomAugmentation:\n",
    "    def __call__(self, x):\n",
    "        # Random horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            x = torch.flip(x, [2])\n",
    "        \n",
    "        # Random vertical flip\n",
    "        if random.random() > 0.5:\n",
    "            x = torch.flip(x, [1])\n",
    "        \n",
    "        # Random crop and resize\n",
    "        if x.shape[1] > 100 and x.shape[2] > 100:\n",
    "            crop_size = random.randint(int(0.8 * min(x.shape[1], x.shape[2])), min(x.shape[1], x.shape[2]))\n",
    "            start_h = random.randint(0, x.shape[1] - crop_size)\n",
    "            start_w = random.randint(0, x.shape[2] - crop_size)\n",
    "            x = x[:, start_h:start_h+crop_size, start_w:start_w+crop_size]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "transform = RandomAugmentation()\n",
    "dataset = MultiViewDataset(\n",
    "    imagery_paths=[imagery_path],\n",
    "    indices_paths=[indices_path],\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create data loader\n",
    "batch_size = 4\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BYOLModel(pretrained=True, with_indices=True).to(device)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.Adam(\n",
    "    list(model.online_encoder.parameters()) +\n",
    "    list(model.online_projector.parameters()) +\n",
    "    list(model.predictor.parameters()),\n",
    "    lr=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.train()\n",
    "epochs = 10\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    for batch in pbar:\n",
    "        view1, view2 = batch\n",
    "        view1 = view1.to(device)\n",
    "        view2 = view2.to(device)\n",
    "        \n",
    "        loss, _ = model.byol_loss(view1, view2)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.update_target_network()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({\"Loss\": loss.item()})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(data_loader)\n",
    "    losses.append(avg_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"results/byol_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, imagery_path, indices_path, output_path):\n",
    "    model.eval()\n",
    "    \n",
    "    # Load imagery\n",
    "    with rasterio.open(imagery_path) as src:\n",
    "        imagery = src.read()\n",
    "        meta = src.meta.copy()\n",
    "    \n",
    "    # Load indices\n",
    "    with rasterio.open(indices_path) as src:\n",
    "        indices = src.read()\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    imagery = torch.from_numpy(imagery).float().unsqueeze(0).to(device)\n",
    "    indices = torch.from_numpy(indices).float().unsqueeze(0).to(device)\n",
    "    \n",
    "    # Combine imagery and indices\n",
    "    x = torch.cat([imagery, indices], dim=1)\n",
    "    \n",
    "    # Extract features\n",
    "    with torch.no_grad():\n",
    "        features = model(x)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    features = features.cpu().numpy()[0]\n",
    "    \n",
    "    # Reshape features to match image dimensions\n",
    "    # This is a simplified approach - in a real scenario, you would use a segmentation head\n",
    "    features_reshaped = np.mean(features, axis=0)\n",
    "    features_reshaped = np.reshape(features_reshaped, (1, 1, -1))\n",
    "    \n",
    "    # Update metadata for output\n",
    "    meta.update({\n",
    "        'count': 1,\n",
    "        'dtype': 'float32'\n",
    "    })\n",
    "    \n",
    "    # Save as GeoTIFF\n",
    "    with rasterio.open(output_path, 'w', **meta) as dst:\n",
    "        dst.write(features_reshaped.astype(np.float32))\n",
    "    \n",
    "    print(f\"Inference completed. Results saved to {output_path}\")\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "output_path = \"results/drainage_features.tif\"\n",
    "features = run_inference(model, imagery_path, indices_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "with rasterio.open(imagery_path) as src:\n",
    "    rgb = src.read([1, 2, 3])\n",
    "    rgb = np.transpose(rgb, (1, 2, 0))\n",
    "    rgb = rgb / rgb.max()\n",
    "\n",
    "with rasterio.open(indices_path) as src:\n",
    "    ndvi = src.read(1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(rgb)\n",
    "plt.title(\"RGB Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.imshow(ndvi, cmap='RdYlGn')\n",
    "plt.title(\"NDVI\")\n",
    "plt.colorbar(shrink=0.5)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.im
