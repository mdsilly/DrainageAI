{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FixMatch Semi-Supervised Learning Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the FixMatch semi-supervised learning approach for drainage pipe detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "from models import SemiSupervisedModel\n",
    "from preprocessing import WeakAugmentation, StrongAugmentation, create_augmentation_pair\n",
    "from training import create_fixmatch_dataloaders, create_validation_dataloader, prepare_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's set up the data directories and create data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set paths\n",
    "labeled_dir = '../data/labeled'\n",
    "unlabeled_dir = '../data/unlabeled'\n",
    "val_dir = '../data/validation'\n",
    "\n",
    "# Create data loaders\n",
    "labeled_loader, unlabeled_loader = create_fixmatch_dataloaders(\n",
    "    labeled_dir, unlabeled_dir, batch_size=4, unlabeled_batch_size=16\n",
    ")\n",
    "\n",
    "val_loader = create_validation_dataloader(val_dir, batch_size=4)\n",
    "\n",
    "print(f\"Number of labeled batches: {len(labeled_loader)}\")\n",
    "print(f\"Number of unlabeled batches: {len(unlabeled_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "\n",
    "Let's visualize some examples from the labeled and unlabeled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get a batch of labeled data\n",
    "labeled_batch = next(iter(labeled_loader))\n",
    "labeled_images = labeled_batch['imagery']\n",
    "labels = labeled_batch['labels']\n",
    "\n",
    "# Get a batch of unlabeled data\n",
    "unlabeled_batch = next(iter(unlabeled_loader))\n",
    "unlabeled_images = unlabeled_batch['imagery']\n",
    "\n",
    "# Display labeled data\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i in range(min(4, labeled_images.size(0))):\n",
    "    # Display image\n",
    "    img = labeled_images[i].permute(1, 2, 0).numpy()\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f\"Labeled Image {i+1}\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Display label\n",
    "    lbl = labels[i].squeeze().numpy()\n",
    "    axes[1, i].imshow(lbl, cmap='gray')\n",
    "    axes[1, i].set_title(f\"Label {i+1}\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display unlabeled data\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for i in range(min(4, unlabeled_images.size(0))):\n",
    "    img = unlabeled_images[i].permute(1, 2, 0).numpy()\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(f\"Unlabeled Image {i+1}\")\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Augmentations\n",
    "\n",
    "Let's visualize the weak and strong augmentations used in FixMatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create augmentations\n",
    "weak_aug, strong_aug = create_augmentation_pair()\n",
    "\n",
    "# Get a sample image\n",
    "sample_image = unlabeled_images[0]\n",
    "\n",
    "# Apply augmentations multiple times\n",
    "weak_augmented = [weak_aug(sample_image) for _ in range(4)]\n",
    "strong_augmented = [strong_aug(sample_image) for _ in range(4)]\n",
    "\n",
    "# Display original image\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(sample_image.permute(1, 2, 0).numpy())\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Display weak augmentations\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, img in enumerate(weak_augmented):\n",
    "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "    axes[i].set_title(f\"Weak Augmentation {i+1}\")\n",
    "    axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display strong augmentations\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "for i, img in enumerate(strong_augmented):\n",
    "    axes[i].imshow(img.permute(1, 2, 0).numpy())\n",
    "    axes[i].set_title(f\"Strong Augmentation {i+1}\")\n",
    "    axes[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Initialize Model\n",
    "\n",
    "Now, let's create and initialize the semi-supervised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model = SemiSupervisedModel(pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate FixMatch Loss\n",
    "\n",
    "Let's demonstrate how the FixMatch loss is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get a batch of labeled and unlabeled data\n",
    "labeled_batch = next(iter(labeled_loader))\n",
    "labeled_images, labels = prepare_batch(labeled_batch, device)\n",
    "\n",
    "unlabeled_batch = next(iter(unlabeled_loader))\n",
    "unlabeled_images, _ = prepare_batch(unlabeled_batch, device)\n",
    "\n",
    "# Compute FixMatch loss\n",
    "total_loss, sup_loss, unsup_loss = model.fixmatch_loss(\n",
    "    labeled_images, labels, unlabeled_images, weak_aug, strong_aug\n",
    ")\n",
    "\n",
    "print(f\"Supervised Loss: {sup_loss.item():.4f}\")\n",
    "print(f\"Unsupervised Loss: {unsup_loss.item():.4f}\")\n",
    "print(f\"Total Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Pseudo-Labels\n",
    "\n",
    "Let's visualize the pseudo-labels generated by the model for unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate pseudo-labels\n",
    "with torch.no_grad():\n",
    "    # Apply weak augmentation\n",
    "    weak_aug_images = weak_aug(unlabeled_images.cpu())\n",
    "    weak_aug_images = weak_aug_images.to(device)\n",
    "    \n",
    "    # Generate pseudo-labels\n",
    "    pseudo_outputs = model(weak_aug_images)\n",
    "    \n",
    "    # Generate binary pseudo-labels and confidence mask\n",
    "    pseudo_labels = (pseudo_outputs > 0.5).float()\n",
    "    confidence_mask = (pseudo_outputs > model.confidence_threshold).float()\n",
    "\n",
    "# Display pseudo-labels\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "for i in range(min(4, unlabeled_images.size(0))):\n",
    "    # Display image\n",
    "    img = unlabeled_images[i].cpu().permute(1, 2, 0).numpy()\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f\"Unlabeled Image {i+1}\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Display pseudo-label\n",
    "    pseudo_lbl = pseudo_labels[i].cpu().squeeze().numpy()\n",
    "    axes[1, i].imshow(pseudo_lbl, cmap='gray')\n",
    "    axes[1, i].set_title(f\"Pseudo-Label {i+1}\")\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Display confidence mask\n",
    "    conf_mask = confidence_mask[i].cpu().squeeze().numpy()\n",
    "    axes[2, i].imshow(conf_mask, cmap='viridis')\n",
    "    axes[2, i].set_title(f\"Confidence Mask {i+1}\")\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Example\n",
    "\n",
    "Let's demonstrate a simple training loop for FixMatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up training parameters\n",
    "num_iterations = 10  # Small number for demonstration\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Create augmentations\n",
    "weak_aug, strong_aug = create_augmentation_pair()\n",
    "\n",
    "# Initialize iterators\n",
    "labeled_iter = iter(labeled_loader)\n",
    "unlabeled_iter = iter(unlabeled_loader)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for iteration in range(num_iterations):\n",
    "    # Get labeled data\n",
    "    try:\n",
    "        labeled_batch = next(labeled_iter)\n",
    "    except StopIteration:\n",
    "        labeled_iter = iter(labeled_loader)\n",
    "        labeled_batch = next(labeled_iter)\n",
    "    \n",
    "    labeled_images, labels = prepare_batch(labeled_batch, device)\n",
    "    \n",
    "    # Get unlabeled data\n",
    "    try:\n",
    "        unlabeled_batch = next(unlabeled_iter)\n",
    "    except StopIteration:\n",
    "        unlabeled_iter = iter(unlabeled_loader)\n",
    "        unlabeled_batch = next(unlabeled_iter)\n",
    "    \n",
    "    unlabeled_images, _ = prepare_batch(unlabeled_batch, device)\n",
    "    \n",
    "    # Compute FixMatch loss\n",
    "    total_loss, sup_loss, unsup_loss = model.fixmatch_loss(\n",
    "        labeled_images, labels, unlabeled_images, weak_aug, strong_aug\n",
    "    )\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Iteration {iteration+1}/{num_iterations}\")\n",
    "    print(f\"  Supervised Loss: {sup_loss.item():.4f}\")\n",
    "    print(f\"  Unsupervised Loss: {unsup_loss.item():.4f}\")\n",
    "    print(f\"  Total Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "\n",
    "Let's evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from training import evaluate_model\n",
    "\n",
    "# Evaluate model\n",
    "metrics = evaluate_model(model, val_loader, device)\n",
    "\n",
    "print(f\"Validation Loss: {metrics['loss']:.4f}\")\n",
    "print(f\"Validation IoU: {metrics['iou']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions\n",
    "\n",
    "Let's visualize the model's predictions on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get a batch of validation data\n",
    "val_batch = next(iter(val_loader))\n",
    "val_images, val_labels = prepare_batch(val_batch, device)\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    val_outputs = model(val_images)\n",
    "    val_predictions = (val_outputs > 0.5).float()\n",
    "\n",
    "# Display predictions\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "\n",
    "for i in range(min(4, val_images.size(0))):\n",
    "    # Display image\n",
    "    img = val_images[i].cpu().permute(1, 2, 0).numpy()\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title(f\"Validation Image {i+1}\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Display ground truth\n",
    "    gt = val_labels[i].cpu().squeeze().numpy()\n",
    "    axes[1, i].imshow(gt, cmap='gray')\n",
    "    axes[1, i].set_title(f\"Ground Truth {i+1}\")\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # Display prediction\n",
    "    pred = val_predictions[i].cpu().squeeze().numpy()\n",
    "    axes[2, i].imshow(pred, cmap='gray')\n",
    "    axes[2, i].set_title(f\"Prediction {i+1}\")\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ensemble Model\n",
    "\n",
    "Finally, let's create an ensemble model that combines the CNN and semi-supervised models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from training import create_ensemble_with_semi, evaluate_ensemble\n",
    "\n",
    "# Save semi-supervised model\n",
    "model_path = '../data/models/semi_model.pt'\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Create ensemble model\n",
    "ensemble = create_ensemble_with_semi(model_path)\n",
    "ensemble.to(device)\n",
    "\n",
    "# Evaluate ensemble model\n",
    "metrics = evaluate_ensemble(ensemble, val_loader, device)\n",
    "\n",
    "print(f\"Ensemble Validation Loss: {metrics['loss']:.4f}\")\n",
    "print(f\"Ensemble Validation IoU: {metrics['iou']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've demonstrated how to use the FixMatch semi-supervised learning approach for drainage pipe detection. We've covered:\n",
    "\n",
    "1. Data preparation and visualization\n",
    "2. Weak and strong augmentations\n",
    "3. FixMatch loss computation\n",
    "4. Pseudo-label generation\n",
    "5. Training loop implementation\n",
    "6. Model evaluation\n",
    "7. Ensemble model creation\n",
    "\n",
    "This approach is particularly useful when you have limited labeled data but access to a large amount of unlabeled data, which is often the case in drainage pipe detection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
